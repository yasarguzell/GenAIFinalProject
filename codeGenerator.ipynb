{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# 1. Load data\n",
        "df = pd.read_csv(\"/insurance.csv\")\n",
        "\n",
        "# 2. Separate features\n",
        "cat_cols = ['sex', 'smoker', 'region']\n",
        "num_cols = ['age', 'bmi', 'children', 'charges']\n",
        "\n",
        "# 3. Preprocessing\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"cat\", OneHotEncoder(sparse_output=False, handle_unknown='ignore'), cat_cols),\n",
        "    (\"num\", StandardScaler(), num_cols)\n",
        "])\n",
        "X = preprocessor.fit_transform(df)\n",
        "\n",
        "input_dim = X.shape[1]\n",
        "latent_dim = 5\n",
        "\n",
        "# 4. ENCODER layer\n",
        "inputs = layers.Input(shape=(input_dim,))\n",
        "h = layers.Dense(32, activation='relu')(inputs)\n",
        "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(h)\n",
        "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(h)\n",
        "\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    eps = tf.random.normal(shape=(tf.shape(z_mean)[0], latent_dim))\n",
        "    return z_mean + tf.exp(0.5 * z_log_var) * eps\n",
        "\n",
        "z = layers.Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
        "\n",
        "encoder = Model(inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "\n",
        "# 5. DECODER layer\n",
        "latent_inputs = layers.Input(shape=(latent_dim,), name='z_sampling')\n",
        "x = layers.Dense(32, activation='relu')(latent_inputs)\n",
        "outputs = layers.Dense(input_dim, activation='sigmoid')(x)\n",
        "\n",
        "decoder = Model(latent_inputs, outputs, name=\"decoder\")\n",
        "\n",
        "# 6. VAE model definition\n",
        "class VAE(Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super(VAE, self).__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n",
        "\n",
        "    def train_step(self, data):\n",
        "        if isinstance(data, tuple):\n",
        "            data = data[0]\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "            reconstruction_loss = tf.reduce_mean(tf.square(data - reconstruction)) * input_dim\n",
        "            kl_loss = -0.5 * tf.reduce_mean(\n",
        "                1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
        "            )\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        return {\"loss\": self.total_loss_tracker.result()}\n",
        "\n",
        "vae = VAE(encoder, decoder)\n",
        "vae.compile(optimizer='adam')\n",
        "\n",
        "# 7. Train the model\n",
        "vae.fit(X, epochs=50, batch_size=32)\n",
        "\n",
        "# 8. Generate new samples\n",
        "z_sample = np.random.normal(size=(10, latent_dim))\n",
        "generated = decoder.predict(z_sample)\n",
        "\n",
        "# 9. Inverse transform numerical features\n",
        "scaler = preprocessor.named_transformers_['num']\n",
        "generated_num = generated[:, -len(num_cols):]  # Numeric features are at the end\n",
        "generated_num_real = scaler.inverse_transform(generated_num)\n",
        "gen_num_df = pd.DataFrame(generated_num_real, columns=num_cols)\n",
        "\n",
        "# 10. Post-process numeric values\n",
        "gen_num_df['age'] = gen_num_df['age'].round().astype(int)\n",
        "gen_num_df['bmi'] = gen_num_df['bmi'].round().astype(int)\n",
        "gen_num_df['children'] = gen_num_df['children'].round().astype(int)\n",
        "gen_num_df['charges'] = gen_num_df['charges'].round(4)\n",
        "\n",
        "# 11. Decode categorical one-hot vectors\n",
        "encoder_cat = preprocessor.named_transformers_['cat']\n",
        "cat_feature_names = encoder_cat.get_feature_names_out(cat_cols)\n",
        "\n",
        "# 12. Calculate lengths of each categorical one-hot group\n",
        "cat_feature_lengths = []\n",
        "for col in cat_cols:\n",
        "    length = sum([1 for name in cat_feature_names if name.startswith(col + '_')])\n",
        "    cat_feature_lengths.append(length)\n",
        "\n",
        "generated_cat = generated[:, :len(cat_feature_names)]  # Categorical part\n",
        "\n",
        "idx = 0\n",
        "decoded_cats = {}\n",
        "for i, col in enumerate(cat_cols):\n",
        "    length = cat_feature_lengths[i]\n",
        "    cat_slice = generated_cat[:, idx:idx+length]\n",
        "    idx += length\n",
        "    cat_labels_idx = np.argmax(cat_slice, axis=1)\n",
        "    categories = encoder_cat.categories_[i]\n",
        "    decoded_cats[col] = categories[cat_labels_idx]\n",
        "\n",
        "gen_cat_df = pd.DataFrame(decoded_cats)\n",
        "\n",
        "# 13. Combine numeric and categorical columns\n",
        "final_df = pd.concat([gen_cat_df, gen_num_df], axis=1)\n",
        "print(final_df)\n"
      ],
      "metadata": {
        "id": "GKiNMgmByWv-",
        "outputId": "e326030e-2a5f-4829-d4c1-35c432aba1d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - loss: 7.1942\n",
            "Epoch 2/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 6.5433\n",
            "Epoch 3/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.9847\n",
            "Epoch 4/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.5826\n",
            "Epoch 5/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.3076\n",
            "Epoch 6/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.1162\n",
            "Epoch 7/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.0146\n",
            "Epoch 8/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0428\n",
            "Epoch 9/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.8791\n",
            "Epoch 10/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.6615\n",
            "Epoch 11/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6059\n",
            "Epoch 12/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.5739\n",
            "Epoch 13/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.4612\n",
            "Epoch 14/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.3396\n",
            "Epoch 15/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.3649\n",
            "Epoch 16/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.3635\n",
            "Epoch 17/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2806\n",
            "Epoch 18/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.2535\n",
            "Epoch 19/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.2634\n",
            "Epoch 20/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.2259\n",
            "Epoch 21/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1826\n",
            "Epoch 22/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1224\n",
            "Epoch 23/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1499\n",
            "Epoch 24/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1513\n",
            "Epoch 25/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1098\n",
            "Epoch 26/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0916\n",
            "Epoch 27/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0860\n",
            "Epoch 28/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0228\n",
            "Epoch 29/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0144\n",
            "Epoch 30/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0583\n",
            "Epoch 31/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9953\n",
            "Epoch 32/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9712\n",
            "Epoch 33/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9884\n",
            "Epoch 34/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9126\n",
            "Epoch 35/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8600\n",
            "Epoch 36/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8423\n",
            "Epoch 37/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9125\n",
            "Epoch 38/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9463\n",
            "Epoch 39/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9423\n",
            "Epoch 40/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9688\n",
            "Epoch 41/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9167\n",
            "Epoch 42/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8905\n",
            "Epoch 43/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8702\n",
            "Epoch 44/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9052\n",
            "Epoch 45/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9522\n",
            "Epoch 46/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9034\n",
            "Epoch 47/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8891\n",
            "Epoch 48/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.8439\n",
            "Epoch 49/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.8930\n",
            "Epoch 50/50\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.9204\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step\n",
            "      sex smoker     region  age  bmi  children       charges\n",
            "0  female     no  southeast   53   33         2  24011.142578\n",
            "1  female     no  southeast   39   31         2  13310.038086\n",
            "2    male    yes  southwest   46   31         2  23869.736328\n",
            "3  female     no  southwest   39   31         1  13273.682617\n",
            "4  female     no  northwest   39   31         1  13271.518555\n",
            "5    male     no  northwest   39   31         1  13270.443359\n",
            "6    male     no  southwest   40   31         2  15049.418945\n",
            "7  female    yes  southeast   39   31         1  25375.298828\n",
            "8  female     no  southeast   39   31         2  13320.573242\n",
            "9  female     no  southwest   42   31         1  13277.074219\n"
          ]
        }
      ]
    }
  ]
}